"""
Evaluation dataset and metrics

This module provides dataset loading, evaluation metrics, and comprehensive analysis.
"""

import json
import re
from typing import List, Dict, Any, Tuple
from collections import Counter
import math


def load_evaluation_dataset(filepath: str) -> List[Dict[str, Any]]:
    """
    Load evaluation dataset from JSON file.
    
    Args:
        filepath: Path to evaluation dataset JSON file
        
    Returns:
        List of test cases
    """
    try:
        with open(filepath, 'r') as f:
            dataset = json.load(f)
        return dataset
    except FileNotFoundError:
        print(f"Dataset file not found: {filepath}")
        return []


def save_evaluation_results(results: List[Dict[str, Any]], filepath: str) -> None:
    """
    Save evaluation results to JSON file.
    
    Args:
        results: List of evaluation results
        filepath: Output filepath
    """
    with open(filepath, 'w') as f:
        json.dump(results, f, indent=2)


def create_sample_dataset() -> List[Dict[str, Any]]:
    """
    Create sample evaluation dataset for testing.
    
    Returns:
        Sample dataset
    """
    return [
        {
            "id": 1,
            "question": "From students attendance find top 5 students with highest attendance and give me their marks in math.",
            "expected_query": "SELECT T2.Name, T1.Math FROM Marks AS T1 JOIN Students AS T2 ON T1.StudentID = T2.StudentID WHERE T2.Name IN (SELECT T4.Name FROM Attendance AS T3 JOIN Students AS T4 ON T3.StudentID = T4.StudentID ORDER BY T3.AttendancePercentage DESC LIMIT 5)",
            "expected_columns": ["Name", "Math"]
        },
        {
            "id": 2,
            "question": "Show me all students with their math marks.",
            "expected_query": "SELECT s.Name, m.Math FROM Students s JOIN Marks m ON s.StudentID = m.StudentID",
            "expected_columns": ["Name", "Math"]
        }
    ]


# ============================================================================
# Advanced Evaluation Metrics
# ============================================================================

def normalize_sql(query: str) -> str:
    """
    Normalize SQL query for comparison (lowercase, remove extra whitespace).
    
    Args:
        query: Raw SQL query
        
    Returns:
        Normalized query string
    """
    # Convert to lowercase
    query = query.lower()
    # Replace multiple whitespaces with single space
    query = re.sub(r'\s+', ' ', query)
    # Remove trailing/leading spaces
    query = query.strip()
    return query


def extract_sql_tokens(query: str) -> List[str]:
    """
    Extract meaningful tokens from SQL query (keywords, tables, columns).
    
    Args:
        query: SQL query string
        
    Returns:
        List of normalized tokens
    """
    normalized = normalize_sql(query)
    # Split by whitespace and common SQL operators
    tokens = re.split(r'[\s,\(\)\*]', normalized)
    # Filter empty tokens
    tokens = [t for t in tokens if t and not t.isspace()]
    return tokens


def calculate_exact_match(generated_query: str, expected_query: str) -> float:
    """
    Calculate exact match score (1.0 if queries are identical, 0.0 otherwise).
    
    Args:
        generated_query: Query generated by LLM
        expected_query: Expected/reference query
        
    Returns:
        Exact match score (0.0 or 1.0)
    """
    return 1.0 if normalize_sql(generated_query) == normalize_sql(expected_query) else 0.0


def calculate_token_match(generated_query: str, expected_query: str) -> float:
    """
    Calculate token-level match score (percentage of matching tokens).
    
    Args:
        generated_query: Query generated by LLM
        expected_query: Expected/reference query
        
    Returns:
        Token match score (0.0 - 1.0)
    """
    generated_tokens = set(extract_sql_tokens(generated_query))
    expected_tokens = set(extract_sql_tokens(expected_query))
    
    if not expected_tokens:
        return 1.0 if not generated_tokens else 0.0
    
    intersection = len(generated_tokens & expected_tokens)
    return intersection / len(expected_tokens)


def calculate_bleu_score(generated_query: str, expected_query: str, n: int = 4) -> float:
    """
    Calculate BLEU score (n-gram overlap metric).
    
    Args:
        generated_query: Query generated by LLM
        expected_query: Expected/reference query
        n: Maximum n-gram size (default: 4)
        
    Returns:
        BLEU score (0.0 - 1.0)
    """
    gen_tokens = extract_sql_tokens(generated_query)
    exp_tokens = extract_sql_tokens(expected_query)
    
    if not exp_tokens or not gen_tokens:
        return 1.0 if exp_tokens == gen_tokens else 0.0
    
    bleu_scores = []
    
    for gram_size in range(1, min(n + 1, len(exp_tokens) + 1)):
        gen_ngrams = Counter([
            tuple(gen_tokens[i:i + gram_size])
            for i in range(len(gen_tokens) - gram_size + 1)
        ])
        exp_ngrams = Counter([
            tuple(exp_tokens[i:i + gram_size])
            for i in range(len(exp_tokens) - gram_size + 1)
        ])
        
        matches = sum((gen_ngrams & exp_ngrams).values())
        total = max(len(gen_tokens) - gram_size + 1, 0)
        
        if total > 0:
            bleu_scores.append(matches / total)
        else:
            bleu_scores.append(0.0)
    
    if not bleu_scores:
        return 0.0
    
    return sum(bleu_scores) / len(bleu_scores)


def calculate_f1_score(generated_query: str, expected_query: str) -> float:
    """
    Calculate F1 score based on token precision and recall.
    
    Args:
        generated_query: Query generated by LLM
        expected_query: Expected/reference query
        
    Returns:
        F1 score (0.0 - 1.0)
    """
    gen_tokens = set(extract_sql_tokens(generated_query))
    exp_tokens = set(extract_sql_tokens(expected_query))
    
    if not exp_tokens and not gen_tokens:
        return 1.0
    
    if not exp_tokens or not gen_tokens:
        return 0.0
    
    intersection = len(gen_tokens & exp_tokens)
    
    # Precision: how many generated tokens were correct
    precision = intersection / len(gen_tokens) if gen_tokens else 0.0
    
    # Recall: how many expected tokens were generated
    recall = intersection / len(exp_tokens) if exp_tokens else 0.0
    
    if precision + recall == 0:
        return 0.0
    
    f1 = 2 * (precision * recall) / (precision + recall)
    return f1


def calculate_semantic_similarity(generated_query: str, expected_query: str) -> float:
    """
    Calculate semantic similarity based on SQL structure (keywords, operations).
    
    Args:
        generated_query: Query generated by LLM
        expected_query: Expected/reference query
        
    Returns:
        Semantic similarity score (0.0 - 1.0)
    """
    # Extract SQL keywords and operations
    sql_keywords = {
        'select', 'from', 'where', 'join', 'inner', 'left', 'right', 'full',
        'on', 'and', 'or', 'order', 'by', 'group', 'limit', 'having',
        'as', 'distinct', 'union', 'intersect', 'except', 'with'
    }
    
    gen_keywords = set(w for w in extract_sql_tokens(generated_query) if w in sql_keywords)
    exp_keywords = set(w for w in extract_sql_tokens(expected_query) if w in sql_keywords)
    
    if not exp_keywords:
        return 1.0 if not gen_keywords else 0.0
    
    intersection = len(gen_keywords & exp_keywords)
    union = len(gen_keywords | exp_keywords)
    
    if union == 0:
        return 0.0
    
    return intersection / union


def calculate_all_metrics(generated_query: str, expected_query: str) -> Dict[str, float]:
    """
    Calculate all available similarity metrics.
    
    Args:
        generated_query: Query generated by LLM
        expected_query: Expected/reference query
        
    Returns:
        Dictionary of metric names and scores
    """
    return {
        "exact_match": calculate_exact_match(generated_query, expected_query),
        "token_match": calculate_token_match(generated_query, expected_query),
        "bleu_score": calculate_bleu_score(generated_query, expected_query),
        "f1_score": calculate_f1_score(generated_query, expected_query),
        "semantic_similarity": calculate_semantic_similarity(generated_query, expected_query),
    }


def calculate_composite_score(metrics: Dict[str, float], weights: Dict[str, float] = None) -> float:
    """
    Calculate composite score from individual metrics.
    
    Args:
        metrics: Dictionary of metric scores
        weights: Dictionary of weights for each metric (defaults to equal weights)
        
    Returns:
        Composite score (0.0 - 1.0)
    """
    if not metrics:
        return 0.0
    
    if weights is None:
        # Equal weights for all metrics
        weights = {key: 1.0 / len(metrics) for key in metrics.keys()}
    
    total_score = sum(metrics.get(key, 0.0) * weights.get(key, 0.0) for key in metrics)
    return total_score


def calculate_sql_correctness(execution_success: bool, is_valid: bool) -> float:
    """
    Calculate SQL correctness score based on validation and execution.
    
    Args:
        execution_success: Whether query executed successfully
        is_valid: Whether query passed syntax validation
        
    Returns:
        Correctness score (0.0 - 1.0)
    """
    if execution_success:
        return 1.0
    elif is_valid:
        return 0.5  # Valid but didn't execute
    else:
        return 0.0  # Invalid syntax


def calculate_metrics_summary(results: List[Dict[str, Any]]) -> Dict[str, Any]:
    """
    Calculate comprehensive metrics summary for all evaluation results.
    
    Args:
        results: List of evaluation results
        
    Returns:
        Summary dictionary with aggregated metrics
    """
    if not results:
        return {}
    
    total = len(results)
    
    # Basic counts
    valid_queries = sum(1 for r in results if r.get("is_valid", False))
    executed = sum(1 for r in results if r.get("execution_success", False))
    errors = sum(1 for r in results if "error" in r and r["error"])
    
    # Similarity metrics
    similarity_metrics = {
        "exact_match": 0.0,
        "token_match": 0.0,
        "bleu_score": 0.0,
        "f1_score": 0.0,
        "semantic_similarity": 0.0,
    }
    
    for result in results:
        if "similarity_metrics" in result:
            for metric_name, value in result["similarity_metrics"].items():
                similarity_metrics[metric_name] += value
    
    # Average the metrics
    for key in similarity_metrics:
        similarity_metrics[key] /= total
    
    summary = {
        "total_tests": total,
        "valid_queries": valid_queries,
        "valid_queries_pct": (valid_queries / total * 100) if total > 0 else 0.0,
        "executed": executed,
        "executed_pct": (executed / total * 100) if total > 0 else 0.0,
        "errors": errors,
        "error_pct": (errors / total * 100) if total > 0 else 0.0,
        "similarity_metrics": similarity_metrics,
        "composite_score": sum(similarity_metrics.values()) / len(similarity_metrics) if similarity_metrics else 0.0,
    }
    
    return summary
